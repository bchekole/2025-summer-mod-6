{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48a74b5e-5180-4544-86a0-b047a877eb8e",
   "metadata": {},
   "source": [
    "# Week 9 - 12 Reflection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36855fce",
   "metadata": {},
   "source": [
    "# Homework Reflection 9\n",
    "\n",
    "1. Write some code that will use a simulation to estimate the standard deviation of the coefficient when there is heteroskedasticity.  \n",
    "Compare these standard errors to those found via statsmodels OLS or a similar linear regression model.\n",
    "\n",
    "\n",
    "- 1 answer. Heteroskedasticity and Standard Error:\n",
    "This part helped me understand why OLS standard errors aren't always reliable. I wrote a simulation using the provided function where the variance of the noise (C) was large, which made the errors heteroskedastic. I then ran 1,000 simulations, each time estimating the coefficient for X, and calculated the standard deviation across those estimates. That gave me a much larger spread than what statsmodels.OLS().fit().bse reported. The key thing I learned is that OLS assumes constant error variance, but in real data—especially when noise increases with a predictor—that assumption breaks. Simulation gave me a more realistic measure of how much the coefficient can vary. It showed me why robust standard errors or simulation-based methods are important when the usual assumptions don’t hold.\n",
    "\n",
    "\n",
    "2. Write some code that will use a simulation to estimate the standard deviation of the coefficient when errors are highly correlated / non-independent.\n",
    "Compare these standard errors to those found via statsmodels OlS or a similar linear regression model.\n",
    "\n",
    "Show that if the correlation between coefficients is high enough, then the estimated standard deviation of the coefficient, using bootstrap errors, \n",
    "might not match that found by a full simulation of the Data Generating Process.  (This can be fixed if you have a huge amount of data for the bootstrap simulation.)\n",
    "\n",
    "- Answer #2 check the code below. When errors are super correlated, bootstrapping might give you a standard error that’s off—and running the full simulation helps expose that. Let me know if you want to add a visual or summary after this!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1da94bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Std Dev from Full DGP Simulation: 0.036654841377710586\n",
      "Std Dev from Bootstrap: 0.03260224253594717\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Parameters\n",
    "A = 1       # True coefficient on X\n",
    "B = 1       # Noise level in X\n",
    "D = 200     # Smaller sample size\n",
    "num_simulations = 1000\n",
    "\n",
    "# --- Strongly Correlated Errors: Full DGP Simulation ---\n",
    "dgp_coefs_strong = []\n",
    "\n",
    "for _ in range(num_simulations):\n",
    "    shared_error = np.random.normal(0, 1)  # Single error shared across all obs\n",
    "    W = np.random.normal(0, 1, D)\n",
    "    X = W + np.random.normal(0, B, D)\n",
    "    Y = A * X - W + shared_error  # Same error value reused → high correlation\n",
    "\n",
    "    X_mat = sm.add_constant(X)\n",
    "    model = sm.OLS(Y, X_mat).fit()\n",
    "    dgp_coefs_strong.append(model.params[1])\n",
    "\n",
    "# --- Bootstrap Residuals on One Sample with Correlation ---\n",
    "W = np.random.normal(0, 1, D)\n",
    "X = W + np.random.normal(0, B, D)\n",
    "shared_error = np.random.normal(0, 1)\n",
    "Y = A * X - W + shared_error\n",
    "\n",
    "X_mat = sm.add_constant(X)\n",
    "model = sm.OLS(Y, X_mat).fit()\n",
    "\n",
    "residuals = model.resid\n",
    "fitted = model.fittedvalues\n",
    "\n",
    "bootstrap_coefs_strong = []\n",
    "for _ in range(1000):\n",
    "    boot_resid = resample(residuals)\n",
    "    Y_boot = fitted + boot_resid\n",
    "    boot_model = sm.OLS(Y_boot, X_mat).fit()\n",
    "    bootstrap_coefs_strong.append(boot_model.params[1])\n",
    "\n",
    "# --- Compare Standard Deviations ---\n",
    "std_dgp_strong = np.std(dgp_coefs_strong)\n",
    "std_boot_strong = np.std(bootstrap_coefs_strong)\n",
    "\n",
    "print(\"Std Dev from Full DGP Simulation:\", std_dgp_strong)\n",
    "print(\"Std Dev from Bootstrap:\", std_boot_strong)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896da167",
   "metadata": {},
   "source": [
    "At first, I wasn’t really sure how correlation between errors could mess with standard errors, but after running the simulations, it started to make sense. I created a setup where the same error was shared across all observations—basically maxing out the correlation. I also used a smaller sample size (D = 200) to make the difference more noticeable.\n",
    "\n",
    "When I compared the standard deviation of X’s coefficient from the full data-generating process (simulating 1,000 datasets) versus bootstrapping residuals from just one sample, I noticed something interesting: they weren’t the same. The bootstrap gave me a smaller standard error than the full simulation.\n",
    "\n",
    "Even though the numbers were kind of close—around 0.0367 vs. 0.0326—the point is that bootstrap assumes each observation is independent, and in this case, they clearly weren’t. That small gap in this example could grow bigger with more complex correlations or if the data were structured differently (like in time series or clustered data).\n",
    "\n",
    "This helped me understand why just bootstrapping residuals isn’t enough when errors are correlated. You either need a huge sample size or a different method that accounts for the correlation directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a220364e",
   "metadata": {},
   "source": [
    "# Quiz 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0e3358a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Effects:\n",
      " C(time)[T.1]     2.252281\n",
      "C(time)[T.2]     3.804417\n",
      "C(time)[T.3]     4.618865\n",
      "C(time)[T.4]     5.492543\n",
      "C(time)[T.5]     6.562369\n",
      "C(time)[T.6]     6.190129\n",
      "C(time)[T.7]     5.755543\n",
      "C(time)[T.8]     5.515259\n",
      "C(time)[T.9]     5.255354\n",
      "C(time)[T.10]    2.960641\n",
      "C(time)[T.11]    1.931654\n",
      "dtype: float64\n",
      "City Effects:\n",
      " C(city)[T.1]    -5.556128\n",
      "C(city)[T.2]    -2.941539\n",
      "C(city)[T.3]     2.003789\n",
      "C(city)[T.4]    -0.234660\n",
      "C(city)[T.5]   -11.019173\n",
      "C(city)[T.6]    -2.891878\n",
      "C(city)[T.7]    -7.678862\n",
      "C(city)[T.8]    -7.502135\n",
      "C(city)[T.9]    -5.336857\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Load the dataset (make sure to use the correct path and quotes)\n",
    "df = pd.read_csv(\"homework_10.1.csv\")\n",
    "\n",
    "# Fit fixed effects model using city and time\n",
    "model = smf.ols('y ~ C(time) + C(city)', data=df).fit()\n",
    "\n",
    "# Fixed effects for each time\n",
    "time_effects = model.params.filter(like='C(time)')\n",
    "print(\"Time Effects:\\n\", time_effects)\n",
    "\n",
    "# Fixed effects for each city\n",
    "city_effects = model.params.filter(like='C(city)')\n",
    "print(\"City Effects:\\n\", city_effects)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bf8b58",
   "metadata": {},
   "source": [
    "The fixed effects increase from time 0 to around 5–6, then decrease again, matching Option A from Question 1.\n",
    "\n",
    "The values jump around without a clear trend — they vary randomly, matching Option B from Question 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1d386e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients for X and Z: [1.47335061 1.69822166]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Generate synthetic data\n",
    "num = 10000\n",
    "X = np.clip(np.random.normal(3, 1, (num,)), 0.01, 100)\n",
    "Z = np.clip(np.random.normal(3, 1, (num,)), 0.01, 100)\n",
    "Y = np.log(X + Z) + np.random.normal(0, 1, (num,))\n",
    "Y_exp = np.exp(Y)\n",
    "\n",
    "# Fit linear regression model: Y_exp ~ X + Z\n",
    "X_matrix = np.column_stack((X, Z))\n",
    "model_exp = LinearRegression().fit(X_matrix, Y_exp)\n",
    "\n",
    "# Show coefficients\n",
    "print(\"Coefficients for X and Z:\", model_exp.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d717a69",
   "metadata": {},
   "source": [
    "The model learns that both X and Z contribute roughly equally and significantly to the outcome. Coefficients around 1.6 reflect that they both scale the target in similar magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0b62197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard error (method i): 0.03306938406909241\n",
      "Simulated std dev (method ii): 0.061233116342809896\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# One-time regression to get standard error\n",
    "np.random.seed(0)\n",
    "num = 10000\n",
    "Z = np.random.normal(0, 1, (num,))\n",
    "X = Z + np.random.normal(0, 1, (num,))\n",
    "Y = 1.5 * X + 2.3 * Z + np.random.normal(0, X**2, (num,))\n",
    "\n",
    "reg1 = sm.OLS(Y, sm.add_constant(np.column_stack((X, Z)))).fit()\n",
    "se_i = reg1.bse[1]  # standard error of X's coefficient\n",
    "\n",
    "# Simulation: estimate std. dev. of X’s coefficient from 100 runs\n",
    "coefficients = []\n",
    "for _ in range(100):\n",
    "    Z = np.random.normal(0, 1, (num,))\n",
    "    X = Z + np.random.normal(0, 1, (num,))\n",
    "    Y = 1.5 * X + 2.3 * Z + np.random.normal(0, X**2, (num,))\n",
    "    reg = sm.OLS(Y, sm.add_constant(np.column_stack((X, Z)))).fit()\n",
    "    coefficients.append(reg.params[1])\n",
    "\n",
    "se_ii = np.std(coefficients)\n",
    "\n",
    "print(\"Standard error (method i):\", se_i)\n",
    "print(\"Simulated std dev (method ii):\", se_ii)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6069862c",
   "metadata": {},
   "source": [
    "Two ways were used to estimate the standard error of X’s coefficient (which is supposed to be 1.5):\n",
    "\n",
    "Method i: Single model fit using .bse\n",
    "Result: ≈ 0.033\n",
    "\n",
    "Method ii: Repeated the data simulation 100 times and calculated the standard deviation of the estimated X coefficients\n",
    "Result: ≈ 0.061\n",
    "\n",
    "Interpretation:\n",
    "Standard errors calculated by repeated simulation (Method ii) capture variability from the data-generating process, including the heteroskedastic noise. That’s why the simulation-based standard deviation is higher.\n",
    "\n",
    "- (ii) is significantly bigger than (i), but neither is zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9d1d93",
   "metadata": {},
   "source": [
    "# Homework Reflection 11\n",
    "\n",
    "1. Construct a dataset for an event study where the value, derivative, and second derivative of a trend all change discontinuously (suddenly) after an event.\n",
    "Build a model that tries to decide whether the event is real (has a nonzero effect) using:\n",
    "(a) only the value,\n",
    "(b) the value, derivative, and second derivative.\n",
    "Which of these models is better at detecting and/or quantifying the impact of the event?  (What might \"better\" mean here?)\n",
    "\n",
    "2. Construct a dataset in which there are three groups whose values each increase discontinuously (suddenly) by the same amount at a shared event; they change in parallel\n",
    "over time, but they have different starting values.  Create a model that combines group fixed effects with an event study, as suggested in the online reading.\n",
    "Explain what you did, how the model works, and how it accounts for both baseline differences and the common event effect."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
